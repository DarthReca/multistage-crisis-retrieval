{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac20849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import spacy\n",
    "\n",
    "from funct.utils import *\n",
    "from funct.data_loading import *\n",
    "from funct.data_processing import *\n",
    "from funct.data_manipulation import *\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*compiled with flash attention.*\")\n",
    "set_manual_seed()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# check if spaCy model is already downloaded\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_md\")\n",
    "    nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12908c94",
   "metadata": {},
   "source": [
    "#### Make sure to run the code on \"cuda\" runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e993fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on {device}')\n",
    "\n",
    "sentence_retriver = SentenceTransformer('BAAI/bge-m3', device=device)\n",
    "cross_encoder = CrossEncoder('mixedbread-ai/mxbai-rerank-xsmall-v1', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a200897",
   "metadata": {},
   "source": [
    "#### Change the event list to your own event list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_no_list_all = [\"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\", \"010\", \"011\", \"012\", \"013\", \"014\", \"015\", \"016\", \"017\", \"018\"]\n",
    "\n",
    "# used for testing\n",
    "# event_no_list_all = [\"018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_day(day, event_no, event_data, sentence_retriever_list, cross_encoder_list, topic_model_list, log):\n",
    "    \"\"\" Process a single day of data for a specific event number.\"\"\"\n",
    "    \n",
    "    # retrieve the components\n",
    "    sentence_retriver = sentence_retriever_list\n",
    "    cross_encoder = cross_encoder_list\n",
    "    topic_model = topic_model_list\n",
    "    \n",
    "    # load the data for the day\n",
    "    dataAsDF = event_data[day]['data']\n",
    "    \n",
    "    # get the search text and sanitize the 'text' column\n",
    "    dataAsDF['search_text'] = dataAsDF['text'].apply(sanitize_data)\n",
    "    \n",
    "    # prepare queries for processing\n",
    "    queryAsDF = prepare_queries(queryAsDF=event_data[day]['query'])\n",
    "    \n",
    "    # filter out the redundant data\n",
    "    filtered_data = filter_redundant_data(dataAsDF=dataAsDF, event_no=event_no, day=day)\n",
    "\n",
    "    # add default columns\n",
    "    add_default_columns(filtered_data)\n",
    "    \n",
    "    if len(filtered_data) == 0:\n",
    "        if log:\n",
    "            print(\"No data found after filtering\")\n",
    "        return\n",
    "    \n",
    "    if len(filtered_data) > 100:\n",
    "        # first step of topic modeling\n",
    "        try:\n",
    "            relevant_topics = assign_topics(filtered_data=filtered_data, topic_model=topic_model)\n",
    "            relevant_data_topic_1 = select_relevant_topics(filtered_data=relevant_topics, \n",
    "                                                queryAsDF=queryAsDF, \n",
    "                                                sentence_retriever=sentence_retriver, \n",
    "                                                topic_model=topic_model, \n",
    "                                                top_percentile=50, \n",
    "                                                threshold=0.5, \n",
    "                                                print_details=log, \n",
    "                                                add_outliers=True,) \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            if log:\n",
    "                print(f\"Error in first step of topic modeling: {e}\")\n",
    "            relevant_data_topic_1 = filtered_data\n",
    "    else:\n",
    "        relevant_data_topic_1 = filtered_data\n",
    "    \n",
    "    if len(relevant_data_topic_1) == 0:\n",
    "        if log:\n",
    "            print(\"No relevant data found\")\n",
    "        return\n",
    "    \n",
    "    # applied BM25 + CrossEncoder to further filter the data\n",
    "    top_k = 200\n",
    "    selected_data_bm25 = process_selection_event_day(dataAsDF=relevant_data_topic_1, \n",
    "                                                    queryAsDF=queryAsDF, \n",
    "                                                    sentence_retriver=sentence_retriver, \n",
    "                                                    cross_encoder=cross_encoder, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    event_no=event_no, \n",
    "                                                    device=device, \n",
    "                                                    top=top_k)\n",
    "        \n",
    "    if len(selected_data_bm25) == 0:\n",
    "        if log:\n",
    "            print(\"No data found after BM25 + CrossEncoder\")\n",
    "        return\n",
    "    \n",
    "    # remove added columns to reset the data selection and add them again for the next step\n",
    "    remove_default_columns(selected_data_bm25)\n",
    "    add_default_columns(selected_data_bm25)\n",
    "    \n",
    "    top_k = min(200, int(len(selected_data_bm25) * 0.1))\n",
    "    \n",
    "    selected_data = process_selection_event_day(dataAsDF=selected_data_bm25, \n",
    "                                                queryAsDF=queryAsDF, \n",
    "                                                sentence_retriver=sentence_retriver, \n",
    "                                                cross_encoder=cross_encoder, \n",
    "                                                retriever_type=\"dense\",\n",
    "                                                batch_size=batch_size, \n",
    "                                                event_no=event_no, \n",
    "                                                dynamic_score=90,\n",
    "                                                device=device, \n",
    "                                                top=top_k)\n",
    "    \n",
    "    selected_data = selected_data.sort_values(by=['cross-score'], ascending=False)\n",
    "\n",
    "    if len(selected_data) == 0:\n",
    "        if log:\n",
    "            print(\"No data found after dense retrieval\")\n",
    "        return\n",
    "    \n",
    "    if len(selected_data) == 0:\n",
    "        if log:\n",
    "            print(\"No data found after dense retrieval\")\n",
    "        return\n",
    "    \n",
    "    save_results(dataframe=selected_data, csv_path=f\"./output/{event_no}_{day}_data.csv\", json_path=f\"./output/results/{event_no}_{day}.json\")\n",
    "    log_statistics(event_no, day, dataAsDF, filtered_data, selected_data_bm25, selected_data, relevant_data_topic_1, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event_day(event_list, sentence_retriever_list, cross_encoder_list, topic_model_list, log):\n",
    "    \"\"\" Process the events and days in the event list.\"\"\"\n",
    "    \n",
    "    for event_no in event_list:\n",
    "\n",
    "        crisis_dataset = CrisisFactsDataset(event_list=[event_no])\n",
    "        event_data = crisis_dataset.get_dataset_event_no_connection(event_no=event_no)\n",
    "        \n",
    "        for day in event_data:\n",
    "            try:\n",
    "                process_day(day=day, \n",
    "                            event_no=event_no, \n",
    "                            event_data=event_data, \n",
    "                            sentence_retriever_list=sentence_retriever_list, \n",
    "                            cross_encoder_list=cross_encoder_list, \n",
    "                            topic_model_list=topic_model_list, \n",
    "                            log=log)\n",
    "            except Exception as e:\n",
    "                if log:\n",
    "                    print(f\"Error in processing day {day}: {e}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = create_topic_model()\n",
    "\n",
    "process_event_day(event_list=event_no_list_all, \n",
    "                sentence_retriever_list=sentence_retriver, \n",
    "                cross_encoder_list=cross_encoder, \n",
    "                topic_model_list=topic_model, \n",
    "                log=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
