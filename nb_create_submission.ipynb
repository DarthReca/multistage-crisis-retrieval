{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9195503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from funct.data_processing import filter_semantic_similarity_2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_no_list_all = [\"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\", \"010\", \"011\", \"012\", \"013\", \"014\", \"015\", \"016\", \"017\", \"018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folder output\n",
    "folder = \"./output\"\n",
    "\n",
    "# load the embedding model\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\")\n",
    "\n",
    "# load from folder json_files the file \"date_to_request.json\"\n",
    "with open('./json_files/date_to_request.json') as f:\n",
    "    date_to_request = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_select_data(event_no, folder, date_to_request, quantile=0.4):\n",
    "    filtered_data = pd.DataFrame()\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        if not file.startswith(event_no):\n",
    "            continue\n",
    "        \n",
    "        request_id = next(\n",
    "            (days.split(\"_\")[-1] for days in date_to_request[event_no]\n",
    "                if days.replace(\"/\", \"_\").split(\"_\")[2] == file.split(\"_\")[1] \n",
    "            ), None\n",
    "        )\n",
    "        \n",
    "        if not request_id:\n",
    "            continue\n",
    "        \n",
    "        data = pd.read_csv(os.path.join(folder, file))\n",
    "        data['request_id'] = request_id\n",
    "        \n",
    "        if {'cross-score', 'text'}.issubset(data.columns):\n",
    "            data = data.sort_values(by='cross-score', ascending=False).reset_index(drop=True)\n",
    "            filtered_data = pd.concat([filtered_data, data], ignore_index=True)\n",
    "            \n",
    "    if {'cross-score', 'text'}.issubset(filtered_data.columns):\n",
    "        filtered_data = filter_semantic_similarity_2(filtered_data, embedding_model=embedding_model, text_column=\"search_text\", threshold=0.95)\n",
    "        filtered_data = filtered_data.sort_values(by='cross-score', ascending=False).reset_index(drop=True)\n",
    "        top_len = int(len(filtered_data) * quantile)\n",
    "        filtered_data = filtered_data.head(top_len)\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "def create_submission(filtered_data, event_no, char_limit=500, top_facts_limit=7):\n",
    "    rows = []\n",
    "    grouped = filtered_data.groupby(['request_id', 'query_id'])\n",
    "    tot_selected_per_event = 0\n",
    "    for (request_id, query_id), group in grouped:\n",
    "        top_facts = group.sort_values(by='cross-score', ascending=False).head(top_facts_limit)\n",
    "\n",
    "        fact_texts = []\n",
    "        total_chars = 0\n",
    "\n",
    "        for _, fact in top_facts.iterrows():\n",
    "            fact_text = fact[\"text\"]\n",
    "            if total_chars + len(fact_text) + 1 > char_limit:\n",
    "                break\n",
    "            fact_texts.append(fact_text)\n",
    "            total_chars += len(fact_text) + 1  # Account for spaces\n",
    "\n",
    "        # Filter `top_facts` to match selected `fact_texts`\n",
    "        fact_texts_df = top_facts.iloc[:len(fact_texts)]\n",
    "        tot_selected_per_event += len(fact_texts_df)\n",
    "        # Store results\n",
    "        rows.append({\n",
    "            'requestID': request_id,\n",
    "            'factText': \". \".join(fact_texts).replace(\"..\", \".\"),  # Concatenate facts properly\n",
    "            'unixTimestamp': fact_texts_df['unix_timestamp'].iloc[0] if not fact_texts_df.empty else None,\n",
    "            'importance': fact_texts_df['cross-score'].mean() if not fact_texts_df.empty else 0,\n",
    "            'sources': fact_texts_df['doc_id'].tolist() if not fact_texts_df.empty else [],\n",
    "            'streamID': None,\n",
    "            'informationNeeds': [query_id]\n",
    "        })\n",
    "    \n",
    "    # print(f\"Total selected facts for event {event_no}: {tot_selected_per_event}\")\n",
    "    \n",
    "    # sort by importance\n",
    "    rows = sorted(rows, key=lambda x: x['importance'], reverse=True)\n",
    "        \n",
    "    return rows\n",
    "\n",
    "def save_submission(output_data, json_path, gz_path):\n",
    "    with open(json_path, \"w\") as f:\n",
    "        for _, row in tqdm(output_data.iterrows(), total=output_data.shape[0], desc=\"Writing to file\"):\n",
    "            f.write(\"%s\\n\" % json.dumps(dict(row)))\n",
    "    \n",
    "    pd.read_json(json_path, lines=True).to_json(gz_path, orient=\"records\", lines=True, compression=\"gzip\")\n",
    "\n",
    "def process_submission(event_list, quantile=0.5, char_limit=500, top_facts_limit=7):\n",
    "    rows = []\n",
    "    for event_no in tqdm(event_list, desc=\"Processing events\"):\n",
    "        # print(f\"\\nProcessing CrisisFACTS-{event_no}\")\n",
    "        data = load_and_select_data(event_no, folder, date_to_request, quantile=quantile)\n",
    "        # data = select_fact_with_mistral(data)\n",
    "        if data.empty:\n",
    "            # print(f\"No data found for CrisisFACTS-{event_no}\")\n",
    "            continue\n",
    "        rows.extend(create_submission(data, event_no, char_limit=char_limit, top_facts_limit=top_facts_limit))\n",
    "    \n",
    "    output_data = pd.DataFrame(rows)\n",
    "    # sort by importance\n",
    "    if rows:\n",
    "        save_submission(output_data, \"./json_files/submission_test.json\", \"./json_files/Thesis_Retriver.gz\")\n",
    "        print(\"Processing complete! Output saved.\")\n",
    "    else:\n",
    "        print(\"No valid data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_submission(event_no_list_all, quantile=0.5, char_limit=500, top_facts_limit=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
